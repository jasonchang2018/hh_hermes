{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowpark for Python\n",
    "import snowflake.snowpark\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import udf, sproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_params = {\n",
    "    \"user\": \"**********\",\n",
    "    \"password\": \"*********\",\n",
    "    \"account\": \"************\", \n",
    "    \"warehouse\": \"ANALYSIS_WH\",\n",
    "    \"database\" : \"EDWPRODHH\",\n",
    "    \"schema\" : \"HERMES\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpark_session = Session.builder.configs(cn_params).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The version of package xgboost in the local environment is 1.7.5, which does not fit the criteria for the requirement xgboost. Your UDF might not work when the package version is different between the server and your local environment\n",
      "The version of package cachetools in the local environment is 5.3.0, which does not fit the criteria for the requirement cachetools. Your UDF might not work when the package version is different between the server and your local environment\n"
     ]
    }
   ],
   "source": [
    "snowpark_session.add_packages('snowflake-snowpark-python', 'xgboost', 'pandas', 'numpy', 'joblib', 'cachetools')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_debtor_v1 (session: Session) -> str:\n",
    "    \n",
    "    from joblib import dump\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import r2_score\n",
    "    from xgboost import XGBRegressor\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Read data   \n",
    "    df = session.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM EDWPRODHH.PUB_MBUTLER.MASTER_DIALER_MODEL_DEBTOR\n",
    "    \"\"\").to_pandas()\n",
    "    labels = df[\"DOL_COMMISSION_ATTR\"]\n",
    "    features = df.drop([\"DOL_COMMISSION_ATTR\"], axis = 1)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Define the model\n",
    "    model = XGBRegressor(n_estimators=10, learning_rate=0.1, max_depth = 9, n_jobs=16, random_state=42)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Compute R-squared score\n",
    "    r2score = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Save the model\n",
    "    dump_path = \"/tmp/train_debtor_v1.joblib\"\n",
    "    dump(model, dump_path)\n",
    "    session.file.put(dump_path, \"@prod_models\", overwrite=True)\n",
    "    \n",
    "    print(\"R-squared: \", r2score)\n",
    "    return \"Model trained and saved with R-squared: \" + str(r2score) + \".\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.snowpark.stored_procedure.StoredProcedure at 0x1a5f7caa910>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowpark_session.sproc.register(\n",
    "    func = train_debtor_v1,\n",
    "    name = \"train_debtor_v1\",\n",
    "    replace = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Model trained and saved with R-squared: 0.2527832813566191.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowpark_session.call(\"train_debtor_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "snowpark_session.add_import(\"@edwprodhh.hermes.prod_models/train_debtor_v1.joblib.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The version of package xgboost in the local environment is 1.7.5, which does not fit the criteria for the requirement xgboost. Your UDF might not work when the package version is different between the server and your local environment\n"
     ]
    }
   ],
   "source": [
    "@udf (name = \"prod_predict_v1_debtor\", stage_location = '@prod_models', session = snowpark_session, packages = [\"pandas\", \"joblib\", \"scikit-learn\", \"xgboost\"], replace = True)\n",
    "def predict_Debtor_rev_v1 (inputs: list) -> float:\n",
    "    \n",
    "    import sys\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from joblib import load\n",
    "        \n",
    "        \n",
    "    IMPORT_DIRECTORY_NAME = \"snowflake_import_directory\"\n",
    "    import_dir = sys._xoptions[IMPORT_DIRECTORY_NAME]\n",
    "    \n",
    "    model_file = import_dir + \"train_debtor_v1.joblib.gz\"\n",
    "    model = load(model_file)\n",
    "    \n",
    "    df = pd.DataFrame(\n",
    "        [inputs],\n",
    "        columns = [\n",
    "              'ASSIGNED_AMT',\n",
    "              'DEBT_AGE',\n",
    "               'EXPERIAN_SCORE',\n",
    "                'MEDIAN_HOUSEHOLD_INCOME', \n",
    "               'HAS_PREVIOUS_PAYMENT', \n",
    "                'IS_ONLY_DEBTOR_IN_PACKET',\n",
    "                'PARKING',\n",
    "                'TOLL', \n",
    "                'AI',\n",
    "                'SP', \n",
    "                'HAS_EMAIL'\n",
    "        ]\n",
    "    )\n",
    "    df['EXPERIAN_SCORE'] = pd.to_numeric(df['EXPERIAN_SCORE'], errors='coerce')\n",
    "    df['MEDIAN_HOUSEHOLD_INCOME'] = pd.to_numeric(df['MEDIAN_HOUSEHOLD_INCOME'], errors='coerce')\n",
    "    df['ASSIGNED_AMT'] = pd.to_numeric(df['ASSIGNED_AMT'], errors='coerce')\n",
    "    \n",
    "    y_pred = model.predict(df)[0]\n",
    "    y_pred = np.clip(y_pred, a_min=0, a_max=None)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "7c8ad584655f3115f3faf149af311a0ee2d9136268726a222d4f96b417e9ff9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
